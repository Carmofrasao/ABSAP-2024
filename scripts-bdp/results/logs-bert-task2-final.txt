2024-05-12 12:05:05.736450: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 12:05:05.764898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-12 12:05:06.295263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
['target', 'input_ids', 'token_type_ids', 'attention_mask']
/home/bruno/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/bruno/.local/lib/python3.8/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
	 Epoch: 1
train-bert-base.py:35: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("accuracy")
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.555 | Train Acc: 77.81 | Train f1: 75.35%
	 Val. Loss: 0.503 |  Val. Acc: 80.07 |  val. f1: 79.50%

	 Epoch: 2
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.366 | Train Acc: 85.14 | Train f1: 84.66%
	 Val. Loss: 0.327 |  Val. Acc: 87.26 |  val. f1: 86.61%

	 Epoch: 3
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.279 | Train Acc: 88.19 | Train f1: 88.00%
	 Val. Loss: 0.281 |  Val. Acc: 88.96 |  val. f1: 88.65%

	 Epoch: 4
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.221 | Train Acc: 90.25 | Train f1: 90.18%
	 Val. Loss: 0.250 |  Val. Acc: 89.58 |  val. f1: 89.41%

	 Epoch: 5
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.179 | Train Acc: 91.42 | Train f1: 91.32%
	 Val. Loss: 0.253 |  Val. Acc: 89.50 |  val. f1: 89.59%

	 Epoch: 6
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.156 | Train Acc: 92.36 | Train f1: 92.40%
	 Val. Loss: 0.252 |  Val. Acc: 89.64 |  val. f1: 89.68%

	 Epoch: 7
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.157 | Train Acc: 92.24 | Train f1: 92.29%
	 Val. Loss: 0.252 |  Val. Acc: 89.64 |  val. f1: 89.68%

	 Epoch: 8
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.157 | Train Acc: 92.38 | Train f1: 92.40%
	 Val. Loss: 0.252 |  Val. Acc: 89.64 |  val. f1: 89.68%

	 Epoch: 9
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.155 | Train Acc: 92.38 | Train f1: 92.41%
	 Val. Loss: 0.252 |  Val. Acc: 89.64 |  val. f1: 89.68%

	 Epoch: 10
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/accuracy/accuracy.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
/home/bruno/.local/lib/python3.8/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/f1/f1.py
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
  warnings.warn(
	Train Loss: 0.157 | Train Acc: 92.26 | Train f1: 92.30%
	 Val. Loss: 0.252 |  Val. Acc: 89.64 |  val. f1: 89.68%

{1461: 0.37130650877952576, 2041: 0.25232046842575073, 1058: 0.2918223440647125, 93: 0.14777320623397827, 945: 0.48133033514022827, 1078: 0.45309993624687195, 1887: 0.24033620953559875, 2092: 0.4006461203098297, 2215: 0.25856536626815796, 1565: 0.537015974521637, 88: 0.16019928455352783, 1318: 0.24214774370193481, 1343: 0.3277741074562073, 744: 0.165329247713089, 174: 0.1816137731075287, 911: 0.2301262617111206, 742: 0.21986842155456543, 2416: 0.3113178312778473, 781: 0.41787853837013245, 511: 0.17003726959228516, 1548: 0.1899029165506363, 331: 0.19673863053321838, 2251: 0.2905958294868469, 2154: 0.26786646246910095, 2330: 0.2380816787481308, 554: 0.29381701350212097, 619: 0.3588034212589264, 1793: 0.3096891939640045, 1928: 0.16974952816963196, 1372: 0.5198291540145874, 1837: 0.4054788053035736, 286: 0.42944595217704773, 1592: 0.1639396846294403, 70: 0.1913762241601944, 1542: 0.24416546523571014, 854: 0.489251047372818, 1796: 0.16714732348918915, 128: 0.19073787331581116, 1381: 0.14643660187721252, 1967: 0.22689826786518097, 2182: 0.2370588183403015, 1768: 0.18923114240169525, 1585: 0.24852833151817322, 2322: 0.12019592523574829, 1910: 0.18300233781337738, 544: 0.2728867828845978, 1516: 0.2805112600326538, 1975: 0.21844343841075897, 2184: 0.17957229912281036, 1563: 0.27782711386680603, 474: 0.33052489161491394, 2354: 0.30394917726516724, 1284: 0.16778013110160828, 822: 0.12931467592716217, 1729: 0.4321594536304474, 402: 0.17302446067333221, 2027: 0.5450889468193054, 32: 0.5007351040840149, 1392: 0.4115888178348541, 1188: 0.16546878218650818, 1885: 0.13432277739048004, 926: 0.25383469462394714, 1464: 0.30486980080604553, 1276: 0.4092244803905487, 2018: 0.2503405809402466, 284: 0.19765889644622803, 1727: 0.42232605814933777, 1310: 0.4834256172180176, 1983: 0.17059436440467834, 1606: 0.33930954337120056, 1532: 0.10725930333137512, 1039: 0.40368160605430603, 472: 0.48923569917678833, 620: 0.3278425335884094, 92: 0.23412597179412842, 777: 0.3221832513809204, 853: 0.2284802794456482, 2254: 0.19209277629852295, 1465: 0.454601526260376, 1157: 0.17956845462322235, 151: 0.49791407585144043, 964: 0.165085107088089, 1169: 0.2535251975059509, 1027: 0.2685259282588959, 1178: 0.21849417686462402, 575: 0.39068758487701416, 2111: 0.4753454327583313, 1374: 0.14530543982982635, 867: 0.20559237897396088, 1430: 0.5221951007843018, 1200: 0.20426392555236816, 2223: 0.23300813138484955, 2412: 0.30018603801727295, 931: 0.3334939777851105, 1612: 0.29475945234298706, 2165: 0.16893291473388672, 339: 0.4955124258995056, 2186: 0.23913441598415375, 285: 0.37136802077293396, 2208: 0.457983136177063, 2002: 0.2942221760749817, 1108: 0.17193879187107086, 1572: 0.384456068277359, 170: 0.32877686619758606, 1726: 0.29562047123908997, 612: 0.43506383895874023, 1222: 0.1767846643924713, 2327: 0.2871660590171814, 551: 0.19475750625133514, 760: 0.46143823862075806, 962: 0.2862597405910492, 1049: 0.2082660049200058, 1802: 0.33290213346481323, 1731: 0.32328468561172485, 1590: 0.2835617661476135, 1839: 0.14809919893741608, 497: 0.17249470949172974, 976: 0.2620866000652313, 1786: 0.29801037907600403, 1996: 0.3498079776763916, 2106: 0.2936762571334839, 555: 0.2915158271789551, 1933: 0.18773676455020905, 496: 0.20094437897205353, 316: 0.13422048091888428, 1101: 0.2378186583518982, 1313: 0.4830614924430847, 1349: 0.24528008699417114, 972: 0.2741018533706665, 2189: 0.3625013828277588, 2074: 0.45019301772117615, 971: 0.2741018533706665, 668: 0.5608882308006287, 1766: 0.22365114092826843, 1575: 0.20758779346942902, 394: 0.29998430609703064, 447: 0.19623327255249023, 2058: 0.1634516716003418, 2130: 0.15726619958877563, 335: 0.21696802973747253, 1113: 0.3309973478317261, 1225: 0.30892518162727356, 1550: 0.387840211391449, 1930: 0.49061328172683716, 166: 0.32894280552864075, 1656: 0.2672460973262787, 1987: 0.3513718843460083, 774: 0.20960910618305206, 2260: 0.21315351128578186, 2249: 0.19206026196479797, 181: 0.2569540739059448, 686: 0.21932770311832428, 1864: 0.17274992167949677, 406: 0.46956726908683777, 2278: 0.2548125684261322, 314: 0.14187954366207123, 2204: 0.20919080078601837, 149: 0.2503657341003418, 1822: 0.47032588720321655, 1650: 0.2933405041694641, 711: 0.27712947130203247, 2141: 0.48204752802848816, 883: 0.3824862241744995, 1383: 0.15780073404312134, 1201: 0.19891352951526642, 1801: 0.33288195729255676, 1434: 0.470485657453537, 1853: 0.22971679270267487, 957: 0.18560484051704407, 2351: 0.25918257236480713, 1750: 0.2161519080400467, 1534: 0.4470500349998474, 370: 0.13950039446353912, 1119: 0.3560788333415985, 600: 0.43508368730545044, 657: 0.326774924993515, 672: 0.5491806268692017, 549: 0.3757080137729645, 2166: 0.2242322862148285, 2199: 0.38515621423721313, 2406: 0.36946260929107666, 2081: 0.35245856642723083, 308: 0.1664622575044632, 542: 0.3237355351448059, 923: 0.1763148456811905, 884: 0.42796963453292847, 1345: 0.27998727560043335, 63: 0.1667700707912445, 1680: 0.6606525182723999, 536: 0.2250942587852478, 1387: 0.23302124440670013, 1995: 0.3505023121833801, 2122: 0.3592510223388672, 2138: 0.19652245938777924, 1597: 0.38797426223754883, 1943: 0.26847168803215027, 376: 0.43907272815704346, 1177: 0.21684981882572174, 1082: 0.2734638750553131, 405: 0.2711450755596161, 894: 0.2659595012664795, 1153: 0.17625318467617035, 26: 0.26988863945007324, 459: 0.39190366864204407, 86: 0.1844349056482315, 1226: 0.5305236577987671, 249: 0.15471962094306946, 1440: 0.21003413200378418, 720: 0.2084387093782425, 1403: 0.2677704095840454, 455: 0.23272915184497833, 341: 0.38584086298942566, 967: 0.27073296904563904, 1232: 0.24214068055152893, 270: 0.5336937308311462, 2006: 0.29349422454833984, 429: 0.31365343928337097, 1247: 0.1869443655014038, 310: 0.3169061839580536, 1589: 0.17458531260490417, 129: 0.38756999373435974, 2036: 0.1376534253358841, 1231: 0.24214068055152893, 2346: 0.19857251644134521, 297: 0.3139928877353668, 1526: 0.27475377917289734, 1478: 0.3191184103488922, 1307: 0.5544289350509644, 2127: 0.08693291991949081, 2098: 0.3828026056289673, 875: 0.31768128275871277, 2129: 0.15222150087356567, 359: 0.47946715354919434, 1559: 0.4930034875869751, 395: 0.2715241014957428, 2187: 0.28148210048675537, 1280: 0.21566012501716614, 904: 0.389151394367218, 161: 0.3288258910179138, 2259: 0.20967422425746918, 1047: 0.20969872176647186, 1467: 0.38465622067451477, 1190: 0.18745769560337067, 628: 0.26433655619621277, 2264: 0.34408363699913025, 2169: 0.2744060158729553, 2079: 0.35146015882492065, 2353: 0.3072564899921417, 2173: 0.3722044825553894, 2282: 0.5263462066650391, 457: 0.36408424377441406, 1819: 0.21265071630477905, 227: 0.3938392400741577, 456: 0.23492863774299622, 1224: 0.30826854705810547, 1682: 0.5290611982345581, 2294: 0.20531877875328064, 101: 0.28103700280189514, 2345: 0.27542901039123535, 35: 0.3534705340862274, 467: 0.3853808343410492, 609: 0.21153011918067932, 673: 0.26493513584136963, 2198: 0.38234373927116394, 217: 0.18052929639816284, 277: 0.24712133407592773, 885: 0.14084048569202423, 1154: 0.25739923119544983, 1543: 0.2759186625480652, 2274: 0.39333242177963257, 2379: 0.2726882994174957, 2391: 0.22458364069461823, 989: 0.116140216588974, 1085: 0.17346936464309692, 1884: 0.18904028832912445, 295: 0.22420886158943176, 2338: 0.28384602069854736, 1131: 0.34909722208976746, 1360: 0.6076287627220154, 449: 0.23733188211917877, 2311: 0.3671630322933197, 73: 0.2757340371608734, 2217: 0.18898746371269226, 384: 0.3407845199108124, 96: 0.4088796377182007, 216: 0.2734455466270447, 1527: 0.274774432182312, 1473: 0.16750147938728333, 682: 0.15142281353473663, 1301: 0.24468839168548584, 734: 0.36500272154808044, 1905: 0.20051124691963196, 59: 0.11778667569160461, 1187: 0.1713244915008545, 1338: 0.2315547913312912, 2335: 0.27936050295829773, 1845: 0.25045549869537354, 99: 0.2761533260345459, 2324: 0.13266557455062866, 1749: 0.35821548104286194, 825: 0.1509561985731125, 1127: 0.192734494805336, 783: 0.22069522738456726, 486: 0.37638071179389954, 1026: 0.13285104930400848, 33: 0.15424640476703644, 1347: 0.1559949368238449, 421: 0.28424394130706787, 2375: 0.32642853260040283, 1136: 0.17696014046669006, 1511: 0.4456300139427185, 716: 0.14708739519119263, 1927: 0.1863117814064026, 1944: 0.15885892510414124, 473: 0.5006252527236938, 1950: 0.3576328456401825, 58: 0.18675650656223297, 727: 0.3606451451778412, 222: 0.36964166164398193, 400: 0.17832106351852417, 1246: 0.23171603679656982, 533: 0.15363536775112152, 2332: 0.38802292943000793, 1424: 0.29205122590065, 1032: 0.354178786277771, 558: 0.35853180289268494, 512: 0.1571418046951294, 724: 0.2145669311285019, 797: 0.21576328575611115, 2403: 0.3686804175376892, 676: 0.5663830637931824, 2388: 0.2207580804824829, 1794: 0.25254181027412415, 246: 0.23630274832248688, 2056: 0.3659886419773102, 584: 0.20137697458267212, 1486: 0.3214932680130005, 1638: 0.5192378759384155, 460: 0.3324299454689026, 1811: 0.20450764894485474, 1916: 0.36964768171310425, 1978: 0.22794754803180695, 946: 0.17696237564086914, 2340: 0.4799972176551819, 1681: 0.24887780845165253, 1721: 0.29789280891418457, 229: 0.29907548427581787, 1072: 0.19029483199119568, 1696: 0.2504178583621979, 1988: 0.35248082876205444, 169: 0.3286811113357544, 572: 0.45306113362312317, 2224: 0.35768812894821167, 753: 0.17213916778564453, 1252: 0.38241979479789734, 2342: 0.2876322865486145, 607: 0.25571611523628235, 436: 0.1921800822019577, 664: 0.10793051868677139, 268: 0.5153422355651855, 799: 0.23755452036857605, 586: 0.1981949359178543, 2114: 0.193804532289505, 427: 0.31309494376182556, 2181: 0.22074221074581146, 355: 0.21153771877288818, 2214: 0.38203543424606323, 1783: 0.42701417207717896, 90: 0.3253459334373474, 1180: 0.19551345705986023, 670: 0.2217184156179428, 844: 0.29796189069747925, 1524: 0.4276183247566223, 2280: 0.5380017757415771, 1746: 0.35752055048942566, 2395: 0.15198856592178345, 1792: 0.3413049578666687, 1106: 0.39899611473083496, 1173: 0.25047844648361206, 2012: 0.17798757553100586, 1114: 0.5024912357330322, 1872: 0.20002184808254242, 548: 0.061925552785396576, 1628: 0.276815265417099, 769: 0.21012897789478302, 2269: 0.2794056832790375, 2240: 0.2842744290828705, 2040: 0.2512255907058716, 1405: 0.19977599382400513, 1134: 0.19759851694107056, 667: 0.24310584366321564, 2160: 0.1549716293811798, 2347: 0.4171319901943207, 1132: 0.19759851694107056, 348: 0.2076982706785202, 2132: 0.3186478912830353, 538: 0.4087402820587158, 1577: 0.20436368882656097, 471: 0.20787779986858368, 914: 0.2935769259929657, 955: 0.16215823590755463, 1852: 0.23798426985740662, 345: 0.197566956281662, 272: 0.4574585258960724, 437: 0.1566348820924759, 1675: 0.26466014981269836, 1182: 0.38302987813949585, 868: 0.3181525766849518, 1821: 0.1380738615989685, 560: 0.2227749079465866, 733: 0.2485838234424591, 2390: 0.22785624861717224, 1788: 0.2994232773780823, 852: 0.23747001588344574, 2014: 0.19081537425518036, 1873: 0.19893625378608704, 444: 0.20957158505916595, 354: 0.21098417043685913, 488: 0.3753734827041626, 1767: 0.19626812636852264, 1999: 0.35061144828796387, 360: 0.2868124544620514, 245: 0.46736469864845276, 1720: 0.29683971405029297, 629: 0.24006502330303192, 1130: 0.29844462871551514, 608: 0.24259191751480103, 537: 0.16844232380390167, 368: 0.1382104456424713, 988: 0.21682243049144745, 1854: 0.39457234740257263, 1807: 0.24819417297840118, 334: 0.20591580867767334, 535: 0.38530516624450684, 1608: 0.36931291222572327, 732: 0.24083255231380463, 1935: 0.5796739459037781, 495: 0.2374965101480484, 547: 0.1650165468454361, 665: 0.14821286499500275, 970: 0.486189603805542, 1941: 0.16307130455970764, 2080: 0.1732107400894165, 243: 0.21111488342285156, 224: 0.15220807492733002, 387: 0.24952809512615204, 397: 0.20978012681007385, 611: 0.2553651034832001, 2239: 0.2804888188838959, 2339: 0.28152015805244446, 361: 0.5892219543457031, 1932: 0.1640678495168686, 1573: 0.1981629580259323, 483: 0.2422017604112625, 1248: 0.18456821143627167, 961: 0.1452801525592804, 656: 0.18424837291240692, 439: 0.5372300148010254, 1435: 0.1873723715543747, 1639: 0.19534865021705627, 2167: 0.2252325862646103, 1176: 0.24301843345165253, 2086: 0.4618804454803467, 1747: 0.35726743936538696, 273: 0.5369757413864136, 1523: 0.2842167317867279, 1596: 0.38883066177368164, 905: 0.26862093806266785, 1732: 0.3190547823905945, 1748: 0.35726743936538696, 1683: 0.19003906846046448, 57: 0.19972549378871918, 369: 0.14125198125839233, 130: 0.1898317188024521, 1808: 0.24949686229228973, 147: 0.2473977506160736, 631: 0.24157442152500153, 2394: 0.29070979356765747, 1100: 0.22596359252929688, 776: 0.14368495345115662, 1593: 0.09231710433959961, 780: 0.40448909997940063, 95: 0.2814836800098419, 1281: 0.3638454079627991, 2076: 0.1211155503988266, 2270: 0.25522541999816895, 1931: 0.20098724961280823, 610: 0.34151867032051086, 2281: 0.1829269528388977, 2405: 0.3671857714653015, 2380: 0.2089339792728424, 1844: 0.4051481783390045, 416: 0.28853335976600647, 1409: 0.29688236117362976, 882: 0.36550313234329224, 1031: 0.3533538579940796, 1809: 0.2629958987236023, 782: 0.32054534554481506, 2042: 0.20080101490020752, 1023: 0.30794984102249146, 1574: 0.12778405845165253, 723: 0.3785524070262909, 1186: 0.17485149204730988, 1965: 0.6378204226493835, 1283: 0.1876544952392578, 1115: 0.23541560769081116, 1818: 0.3346749544143677, 2090: 0.40293800830841064, 2001: 0.2950296998023987, 1242: 0.45312029123306274, 627: 0.24658682942390442, 1785: 0.29221978783607483, 2253: 0.29002493619918823, 1373: 0.16985386610031128, 1116: 0.3310033082962036, 282: 0.20360468327999115, 1937: 0.17637775838375092, 72: 0.3679766356945038, 800: 0.22280186414718628, 1962: 0.14193817973136902, 2266: 0.32980769872665405, 1384: 0.17181465029716492, 1981: 0.5076691508293152, 1022: 0.2712884843349457, 164: 0.32895317673683167, 1379: 0.17181465029716492, 494: 0.24565117061138153, 2089: 0.4015091359615326, 2078: 0.22824440896511078, 1518: 0.2877810597419739, 1133: 0.20572659373283386, 1088: 0.29740482568740845, 573: 0.19324934482574463, 1843: 0.24957387149333954, 1911: 0.18640005588531494, 2226: 0.34810706973075867, 1344: 0.2843356430530548, 1179: 0.38302987813949585, 231: 0.30111315846443176, 2238: 0.2842744290828705, 68: 0.42611604928970337, 1874: 0.33327892422676086, 298: 0.3145858347415924, 153: 0.3306056261062622, 1517: 0.28850024938583374, 1228: 0.5161051750183105, 228: 0.10715638101100922, 1936: 0.1988123655319214, 545: 0.281461626291275, 1961: 0.4058300256729126, 550: 0.2626137435436249, 1883: 0.19549691677093506, 34: 0.5178043246269226, 1223: 0.5161051750183105, 1463: 0.2880851924419403, 874: 0.18616566061973572, 933: 0.3220617175102234, 1940: 0.2730088233947754, 446: 0.22180289030075073, 798: 0.24657391011714935, 466: 0.41385984420776367, 62: 0.2128031849861145, 1274: 0.1769697219133377, 2221: 0.2221834510564804, 1610: 0.3788117468357086, 2168: 0.24069292843341827, 1520: 0.28850024938583374, 1997: 0.3489248752593994, 666: 0.2777049243450165, 451: 0.22760610282421112, 1719: 0.30072081089019775, 2374: 0.32732003927230835, 2386: 0.22956976294517517, 1243: 0.23746219277381897, 759: 0.4543400704860687, 2007: 0.4878951907157898, 1838: 0.4055860936641693, 417: 0.4419601857662201, 557: 0.20985294878482819, 1156: 0.33435654640197754, 552: 0.19662430882453918, 476: 0.20437680184841156, 710: 0.17805904150009155, 242: 0.40849727392196655, 785: 0.41853049397468567, 1469: 0.1678338348865509, 1181: 0.2032049000263214, 2247: 0.2649171054363251, 393: 0.2716761529445648, 1202: 0.2064773440361023, 583: 0.20612642168998718, 2205: 0.1872938573360443, 1870: 0.19521360099315643, 1340: 0.2670692205429077, 461: 0.3900507092475891, 1994: 0.3495916724205017, 856: 0.19674919545650482, 2087: 0.4033615291118622, 283: 0.19995269179344177, 1588: 0.20442219078540802, 1790: 0.22702740132808685, 1640: 0.3573770821094513, 1315: 0.3001209497451782, 1951: 0.3556325435638428, 2219: 0.2103194296360016, 895: 0.3439777195453644, 2376: 0.32703983783721924, 2393: 0.2926640212535858, 1823: 0.48294198513031006, 377: 0.3380872309207916, 1024: 0.33313751220703125, 386: 0.5943223834037781, 944: 0.18436414003372192, 795: 0.21284399926662445, 1629: 0.4422151446342468, 175: 0.2905595302581787, 428: 0.29991671442985535, 2271: 0.30309975147247314, 1993: 0.35056769847869873, 2273: 0.3228699862957001, 1080: 0.18608513474464417, 869: 0.20995916426181793, 1611: 0.29300644993782043, 1314: 0.23239818215370178, 1769: 0.1966373175382614, 754: 0.3618120551109314, 269: 0.19550006091594696, 2075: 0.5022969245910645, 1084: 0.3995005488395691, 532: 0.23859688639640808, 125: 0.24636197090148926, 1175: 0.2134474664926529, 541: 0.3236972689628601, 2207: 0.46402138471603394, 606: 0.2555640637874603, 2262: 0.20838721096515656, 2279: 0.5384342074394226, 569: 0.3931964933872223, 508: 0.224215567111969, 726: 0.48790183663368225, 2107: 0.5228535532951355, 2413: 0.22085191309452057, 2349: 0.43356430530548096, 1770: 0.3346695899963379, 1087: 0.2943817377090454, 543: 0.266070157289505, 778: 0.1420912891626358, 389: 0.2501057982444763, 2310: 0.17405655980110168, 55: 0.12478360533714294, 2341: 0.2877484858036041, 177: 0.13779552280902863, 1875: 0.3332742750644684, 1942: 0.2896934449672699, 1871: 0.19383496046066284, 218: 0.17209461331367493, 908: 0.19044709205627441, 385: 0.2501057982444763, 1408: 0.3309459388256073, 1912: 0.14634914696216583, 1251: 0.13697317242622375, 1677: 0.23282700777053833, 968: 0.26920005679130554, 453: 0.23012498021125793, 2352: 0.30936622619628906, 719: 0.40166789293289185, 2295: 0.19792717695236206, 487: 0.3750874698162079, 438: 0.14124274253845215, 2035: 0.3250749409198761, 1810: 0.2582767903804779, 1030: 0.35459673404693604, 2263: 0.20772361755371094, 1249: 0.18894630670547485, 1137: 0.32515183091163635, 2337: 0.28307148814201355, 1388: 0.40351757407188416, 2134: 0.14939455687999725, 221: 0.20228923857212067, 2334: 0.28307148814201355, 225: 0.17795328795909882, 1317: 0.2995518743991852, 346: 0.4445638358592987, 1128: 0.22474665939807892, 846: 0.19934014976024628, 344: 0.256082147359848, 1060: 0.15351317822933197, 1477: 0.5884295105934143, 1120: 0.3551805317401886, 1904: 0.193093940615654, 1525: 0.3295350968837738, 1552: 0.3901577591896057, 2296: 0.20325374603271484, 669: 0.23946303129196167, 301: 0.30382370948791504, 987: 0.216258242726326, 430: 0.31393250823020935, 396: 0.21545088291168213, 2020: 0.3420189321041107, 2265: 0.3442539572715759, 721: 0.2105485498905182, 1184: 0.2096881866455078, 767: 0.20454491674900055, 2000: 0.3498672842979431, 2190: 0.3590063154697418, 1817: 0.18692518770694733, 299: 0.44787368178367615, 305: 0.1801336705684662, 53: 0.14199146628379822, 1399: 0.27165234088897705, 182: 0.44102853536605835, 1949: 0.16456055641174316, 567: 0.30562490224838257, 1057: 0.29129305481910706, 1587: 0.2078871726989746, 419: 0.5177776217460632, 2153: 0.2555128037929535, 546: 0.23806123435497284, 745: 0.16316325962543488, 2377: 0.27452582120895386, 1442: 0.24520596861839294, 1380: 0.1725926548242569, 1591: 0.161883145570755, 89: 0.46501898765563965, 674: 0.43294551968574524, 1025: 0.38121843338012695, 2222: 0.34810447692871094, 1277: 0.38156628608703613, 2328: 0.19547557830810547, 100: 0.28262412548065186, 30: 0.17100508511066437, 1789: 0.2300320416688919, 663: 0.2767776846885681, 69: 0.2554774582386017, 399: 0.3780890107154846, 1309: 0.2039366513490677, 61: 0.4136914312839508, 1842: 0.2490759938955307, 146: 0.1961006075143814, 454: 0.21671634912490845, 1339: 0.270438551902771, 315: 0.2847907841205597, 1404: 0.31816327571868896, 661: 0.23741354048252106, 309: 0.3177882134914398, 671: 0.23001527786254883, 762: 0.17469148337841034, 1676: 0.24009525775909424, 74: 0.17152631282806396, 375: 0.4337482750415802, 1803: 0.2588596045970917, 465: 0.38412994146347046, 1391: 0.4006917178630829, 1441: 0.19230282306671143, 1840: 0.40598008036613464, 150: 0.19022828340530396, 56: 0.14076095819473267, 278: 0.24911679327487946, 91: 0.28519073128700256, 1402: 0.43448111414909363, 244: 0.4068518579006195, 1126: 0.20734483003616333, 178: 0.15246924757957458, 1221: 0.3088400959968567, 219: 0.4021277129650116, 332: 0.19404803216457367, 568: 0.504150927066803, 2028: 0.25523266196250916, 1081: 0.20443095266819, 126: 0.17622949182987213, 2387: 0.24414999783039093, 927: 0.2630106806755066, 131: 0.16813211143016815, 556: 0.3440001308917999, 97: 0.40849384665489197, 1484: 0.08146397769451141, 2112: 0.2522345781326294, 2126: 0.31826865673065186, 1998: 0.34954777359962463, 1227: 0.33046838641166687, 312: 0.17384487390518188, 356: 0.2700653076171875, 1470: 0.16828852891921997, 163: 0.3288482427597046, 2192: 0.3101452887058258, 333: 0.21427640318870544, 1805: 0.2797496020793915, 2415: 0.3104410469532013, 36: 0.2739299237728119, 1820: 0.17025765776634216, 2059: 0.2565184235572815, 570: 0.3644132614135742, 1361: 0.5790078043937683, 1584: 0.20477716624736786, 2025: 0.16366995871067047, 960: 0.15436601638793945, 1576: 0.2111215442419052, 2128: 0.4533705711364746, 1594: 0.28437909483909607, 2392: 0.1629466712474823, 2343: 0.31676533818244934, 77: 0.21045078337192535, 1075: 0.2204473465681076, 300: 0.3129136264324188, 1348: 0.16185835003852844, 485: 0.3750276267528534, 1867: 0.2776511013507843, 1979: 0.1730327606201172, 1562: 0.2477128803730011, 571: 0.5181391835212708, 892: 0.2643943428993225, 340: 0.18561026453971863, 1109: 0.29833468794822693, 1135: 0.1820211559534073, 1155: 0.25317880511283875, 2104: 0.46333980560302734, 1521: 0.36203840374946594, 1561: 0.21305932104587555, 929: 0.24470657110214233, 921: 0.1766388714313507, 871: 0.2980409264564514, 1694: 0.31403225660324097, 1278: 0.38583171367645264, 484: 0.3750276267528534, 1903: 0.18787038326263428, 973: 0.20803816616535187, 925: 0.18176397681236267, 1282: 0.18085698783397675, 1902: 0.18251323699951172, 2329: 0.2283286601305008, 2389: 0.2223316729068756, 2094: 0.4071735739707947, 1112: 0.3573095500469208, 1784: 0.28695765137672424, 845: 0.18684838712215424, 1909: 0.18484118580818176, 71: 0.2032342255115509, 179: 0.24787485599517822, 1411: 0.1807904988527298, 1170: 0.2727803885936737, 2085: 0.18453258275985718, 1038: 0.16565296053886414, 452: 0.26157644391059875, 1121: 0.5113311409950256, 2123: 0.3407604694366455, 823: 0.15161095559597015, 226: 0.1635621339082718, 2344: 0.2561718225479126, 1102: 0.25241297483444214, 1679: 0.2276751548051834, 443: 0.17670691013336182, 1476: 0.19925528764724731, 31: 0.14740842580795288, 1512: 0.38081932067871094, 2225: 0.09319529682397842, 1183: 0.1887049525976181, 223: 0.25329387187957764, 1485: 0.05097160115838051, 1553: 0.3226204514503479, 2331: 0.22243252396583557, 1191: 0.4538937509059906, 1279: 0.17257320880889893, 311: 0.489140123128891, 922: 0.25480884313583374, 2404: 0.3664478063583374, 2005: 0.2958730161190033, 1171: 0.3229186236858368, 148: 0.14883823692798615, 714: 0.24134214222431183, 1444: 0.2451915591955185, 1462: 0.1326696127653122, 1117: 0.2317524254322052, 1185: 0.38302987813949585, 1308: 0.1979428231716156, 1111: 0.19857275485992432, 2084: 0.5475261807441711, 1073: 0.19890998303890228, 924: 0.23508156836032867, 98: 0.2826153337955475, 2140: 0.4850783944129944, 566: 0.36367613077163696, 230: 0.10351129621267319, 2137: 0.3147881329059601, 752: 0.21831072866916656, 76: 0.12817835807800293, 373: 0.33767783641815186, 1797: 0.28108707070350647, 928: 0.16580656170845032, 1980: 0.19159257411956787, 912: 0.2287249118089676, 534: 0.18258368968963623, 1358: 0.31614693999290466, 779: 0.4030248522758484, 539: 0.3245149254798889, 2326: 0.28903746604919434, 2325: 0.1549679934978485, 2283: 0.2570751905441284, 1074: 0.19810278713703156, 342: 0.4593232572078705, 958: 0.1966225653886795, 2419: 0.310878187417984, 25: 0.2733142673969269, 2083: 0.45345205068588257, 445: 0.195737823843956, 553: 0.1994963437318802, 294: 0.17908857762813568, 1522: 0.3962954580783844, 1250: 0.20824111998081207, 1963: 0.2872737944126129, 870: 0.2104458212852478, 2197: 0.39912551641464233, 1359: 0.4180428683757782, 2418: 0.310878187417984, 2057: 0.3641948997974396, 1560: 0.4891156852245331, 621: 0.35930290818214417, 1841: 0.40598008036613464, 2191: 0.3591645658016205, 162: 0.3289276361465454, 930: 0.25110164284706116, 2091: 0.40191343426704407, 1341: 0.2841953635215759, 165: 0.3289276361465454, 475: 0.20711416006088257, 357: 0.20954295992851257, 1172: 0.256856769323349, 1037: 0.21362151205539703, 60: 0.4136914312839508, 2109: 0.25091561675071716, 1229: 0.24909965693950653, 1103: 0.18152134120464325, 963: 0.2862597405910492, 1514: 0.1810646653175354, 2420: 0.3100467622280121, 1245: 0.45549464225769043, 2218: 0.5098970532417297, 2237: 0.28222981095314026, 1244: 0.23162208497524261, 1337: 0.2680915892124176, 401: 0.21590103209018707, 1934: 0.5677767395973206, 418: 0.45755913853645325, 722: 0.4301051199436188, 1929: 0.1983201503753662, 910: 0.22417514026165009, 1048: 0.2082660049200058, 1382: 0.16508692502975464, 1443: 0.1873815506696701, 824: 0.16411426663398743, 22: 0.25365644693374634, 1129: 0.3000720143318176, 2201: 0.38293206691741943, 675: 0.29018738865852356, 2312: 0.2544344663619995, 1487: 0.2776762545108795, 1765: 0.21915948390960693, 1398: 0.31760773062705994, 975: 0.20700117945671082, 623: 0.2035519778728485, 1076: 0.171818345785141, 715: 0.33985036611557007, 1159: 0.22481676936149597, 2378: 0.203449547290802, 1410: 0.33099254965782166, 1230: 0.2417547106742859, 2038: 0.19520063698291779, 768: 0.3536204695701599, 398: 0.18661914765834808, 630: 0.3853050768375397, 2096: 0.2294527143239975, 2017: 0.24879767000675201, 605: 0.2574317455291748, 1541: 0.1692206859588623, 2385: 0.24055063724517822, 2003: 0.2933206260204315, 2054: 0.3659886419773102, 1886: 0.27920156717300415, 378: 0.18174251914024353, 2004: 0.2933206260204315, 302: 0.18499235808849335, 2174: 0.21949784457683563, 559: 0.20546719431877136, 1728: 0.42402002215385437, 306: 0.1699977070093155, 2093: 0.4058205783367157, 2250: 0.23831196129322052, 2113: 0.44502905011177063, 1513: 0.2719334363937378, 1431: 0.18893671035766602, 1913: 0.44450268149375916, 2037: 0.1698140949010849, 2227: 0.24691267311573029, 1194: 0.22003503143787384, 2082: 0.5638515949249268, 1104: 0.25968649983406067, 1406: 0.19462746381759644, 1549: 0.26543647050857544, 2055: 0.2554665505886078, 1806: 0.23240384459495544, 1804: 0.2332446575164795, 736: 0.2453773021697998, 2350: 0.26238882541656494, 281: 0.2857998311519623, 1389: 0.23517677187919617, 2152: 0.26424622535705566, 2200: 0.40127983689308167, 574: 0.3044886589050293, 2414: 0.30038949847221375, 1107: 0.25968649983406067, 1982: 0.5800999999046326, 2297: 0.2850123643875122, 450: 0.2526189684867859, 2103: 0.13547497987747192, 349: 0.3121536374092102, 1630: 0.2024519294500351, 1799: 0.1915282905101776, 601: 0.38166043162345886, 2396: 0.16055406630039215, 2183: 0.19433711469173431, 622: 0.3601703345775604, 2216: 0.1384340524673462, 2336: 0.33837875723838806, 1192: 0.17828820645809174, 585: 0.34569403529167175, 247: 0.20776352286338806, 1519: 0.282120019197464, 271: 0.19150874018669128, 1110: 0.16375762224197388, 374: 0.25277331471443176, 388: 0.2473934441804886, 2228: 0.3500228226184845, 168: 0.32843631505966187, 276: 0.23930048942565918, 1050: 0.19826309382915497, 1800: 0.33286601305007935, 2155: 0.1674112230539322, 1787: 0.16787320375442505, 2162: 0.17409320175647736, 1077: 0.46933451294898987, 1976: 0.22200451791286469, 296: 0.21427232027053833, 152: 0.33026939630508423, 872: 0.20620574057102203, 2323: 0.3839268386363983, 1189: 0.16157785058021545, 2039: 0.18763552606105804, 2024: 0.1449565440416336, 1595: 0.3910866379737854, 2213: 0.24844186007976532, 1407: 0.20959194004535675, 2125: 0.34062090516090393, 2272: 0.2994318902492523, 2019: 0.3426685929298401, 2105: 0.29061445593833923, 2110: 0.24461376667022705, 422: 0.3092155158519745, 1795: 0.2751612067222595, 893: 0.26280683279037476, 1586: 0.4447842538356781, 796: 0.21082955598831177, 1751: 0.15721364319324493, 1554: 0.2372131496667862, 1915: 0.44900286197662354, 1678: 0.23071709275245667, 2333: 0.15220347046852112, 2034: 0.32489651441574097, 2255: 0.2851507365703583, 2246: 0.233308807015419, 855: 0.18584267795085907, 725: 0.38241416215896606, 2133: 0.5340905785560608, 1471: 0.1929362267255783, 662: 0.4591095745563507, 1319: 0.23066513240337372, 2088: 0.4088405668735504, 2348: 0.22018590569496155, 1791: 0.362541526556015, 613: 0.25427573919296265, 1564: 0.18029876053333282, 932: 0.15770398080348969, 1086: 0.5350854396820068, 2124: 0.316977858543396, 1865: 0.2763808071613312, 1118: 0.3565739095211029, 1964: 0.24055255949497223, 1285: 0.14457491040229797, 280: 0.2824927270412445, 510: 0.45741167664527893, 956: 0.1436125487089157, 2248: 0.5356749892234802, 1275: 0.4058753550052643, 2206: 0.41386526823043823, 775: 0.32153454422950745, 2175: 0.17228007316589355, 909: 0.21701587736606598, 127: 0.2450326830148697, 2164: 0.13522475957870483, 1984: 0.5086696743965149, 1316: 0.2309049516916275, 969: 0.32905787229537964, 274: 0.193480983376503, 180: 0.25054988265037537, 54: 0.19586341083049774, 307: 0.16328725218772888, 492: 0.3072919547557831, 540: 0.3235543370246887, 2108: 0.48407936096191406, 509: 0.3672827482223511, 743: 0.2202385663986206, 1436: 0.18837293982505798, 1346: 0.27897587418556213, 1876: 0.33327317237854004, 1693: 0.3136235475540161, 87: 0.17595051229000092, 2252: 0.16600927710533142, 1468: 0.15517835319042206, 493: 0.1623074859380722, 248: 0.1467318832874298, 1105: 0.2549338638782501, 2185: 0.18664327263832092, 2161: 0.15027934312820435, 390: 0.340753972530365, 2136: 0.31430643796920776, 2163: 0.15027934312820435, 1866: 0.3882482945919037, 2417: 0.3104247450828552, 358: 0.4788132309913635, 275: 0.45240095257759094, 215: 0.5717213153839111, 1966: 0.2479420006275177, 913: 0.22594492137432098, 876: 0.3427814841270447, 2261: 0.21025308966636658, 1400: 0.26999980211257935, 709: 0.5169891119003296, 1371: 0.20318551361560822, 279: 0.2474779635667801, 965: 0.19896666705608368, 1816: 0.2465081363916397, 1855: 0.23555737733840942, 1730: 0.22041508555412292, 708: 0.1783866435289383, 1695: 0.2535471022129059, 420: 0.45553234219551086, 758: 0.1951536238193512, 220: 0.22931760549545288, 23: 0.24597479403018951, 1914: 0.4828513562679291, 1977: 0.3645690381526947, 2220: 0.3696223497390747, 873: 0.5055446624755859, 2115: 0.19383841753005981, 1342: 0.2839926481246948, 1079: 0.17675964534282684, 24: 0.2730655372142792, 1472: 0.5206387042999268, 1193: 0.2548105716705322, 1798: 0.2547227740287781, 1901: 0.21352840960025787, 448: 0.19099271297454834, 959: 0.15965069830417633, 1641: 0.3569416105747223, 2095: 0.528792142868042, 176: 0.17811977863311768, 2097: 0.4192712604999542, 1390: 0.33397552371025085, 1863: 0.2814551591873169, 435: 0.23493865132331848, 1533: 0.4389692544937134, 167: 0.3289109766483307, 2026: 0.2549312710762024, 2135: 0.5160175561904907, 1401: 0.43489277362823486, 1083: 0.41362494230270386, 1824: 0.3886168897151947, 1393: 0.33397552371025085}

{1461: 2, 2041: 0, 1058: 2, 93: 0, 945: 1, 1078: 2, 1887: 0, 2092: 2, 2215: 2, 1565: 1, 88: 2, 1318: 0, 1343: 0, 744: 2, 174: 2, 911: 0, 742: 0, 2416: 0, 781: 1, 511: 2, 1548: 2, 331: 0, 2251: 0, 2154: 0, 2330: 0, 554: 0, 619: 2, 1793: 2, 1928: 2, 1372: 1, 1837: 0, 286: 1, 1592: 2, 70: 2, 1542: 2, 854: 2, 1796: 2, 128: 2, 1381: 2, 1967: 0, 2182: 0, 1768: 0, 1585: 2, 2322: 2, 1910: 2, 544: 0, 1516: 2, 1975: 2, 2184: 0, 1563: 2, 474: 0, 2354: 0, 1284: 2, 822: 2, 1729: 1, 402: 0, 2027: 1, 32: 1, 1392: 0, 1188: 0, 1885: 2, 926: 2, 1464: 2, 1276: 0, 2018: 2, 284: 0, 1727: 1, 1310: 1, 1983: 2, 1606: 2, 1532: 2, 1039: 0, 472: 1, 620: 2, 92: 2, 777: 2, 853: 2, 2254: 0, 1465: 2, 1157: 2, 151: 1, 964: 2, 1169: 0, 1027: 2, 1178: 2, 575: 1, 2111: 2, 1374: 2, 867: 0, 1430: 1, 1200: 0, 2223: 0, 2412: 0, 931: 2, 1612: 0, 2165: 0, 339: 1, 2186: 0, 285: 0, 2208: 1, 2002: 0, 1108: 2, 1572: 2, 170: 2, 1726: 2, 612: 2, 1222: 2, 2327: 2, 551: 2, 760: 2, 962: 2, 1049: 0, 1802: 0, 1731: 0, 1590: 2, 1839: 0, 497: 2, 976: 2, 1786: 2, 1996: 2, 2106: 2, 555: 0, 1933: 2, 496: 2, 316: 2, 1101: 2, 1313: 1, 1349: 0, 972: 2, 2189: 0, 2074: 2, 971: 2, 668: 1, 1766: 0, 1575: 2, 394: 2, 447: 0, 2058: 2, 2130: 0, 335: 0, 1113: 0, 1225: 2, 1550: 2, 1930: 1, 166: 2, 1656: 0, 1987: 2, 774: 2, 2260: 0, 2249: 0, 181: 2, 686: 0, 1864: 2, 406: 1, 2278: 0, 314: 2, 2204: 0, 149: 2, 1822: 1, 1650: 0, 711: 2, 2141: 1, 883: 2, 1383: 2, 1201: 0, 1801: 0, 1434: 1, 1853: 2, 957: 2, 2351: 2, 1750: 2, 1534: 1, 370: 2, 1119: 0, 600: 0, 657: 0, 672: 1, 549: 2, 2166: 0, 2199: 0, 2406: 0, 2081: 1, 308: 0, 542: 0, 923: 2, 884: 2, 1345: 0, 63: 2, 1680: 1, 536: 0, 1387: 0, 1995: 2, 2122: 0, 2138: 2, 1597: 2, 1943: 2, 376: 2, 1177: 2, 1082: 2, 405: 2, 894: 2, 1153: 0, 26: 0, 459: 1, 86: 0, 1226: 1, 249: 2, 1440: 2, 720: 2, 1403: 0, 455: 0, 341: 1, 967: 2, 1232: 2, 270: 1, 2006: 0, 429: 0, 1247: 2, 310: 0, 1589: 2, 129: 2, 2036: 2, 1231: 2, 2346: 2, 297: 0, 1526: 0, 1478: 2, 1307: 1, 2127: 2, 2098: 2, 875: 0, 2129: 0, 359: 2, 1559: 1, 395: 0, 2187: 0, 1280: 0, 904: 0, 161: 2, 2259: 0, 1047: 0, 1467: 1, 1190: 0, 628: 0, 2264: 0, 2169: 0, 2079: 1, 2353: 0, 2173: 1, 2282: 1, 457: 2, 1819: 0, 227: 2, 456: 0, 1224: 2, 1682: 1, 2294: 0, 101: 0, 2345: 0, 35: 2, 467: 0, 609: 2, 673: 0, 2198: 0, 217: 2, 277: 2, 885: 2, 1154: 0, 1543: 2, 2274: 0, 2379: 0, 2391: 0, 989: 2, 1085: 2, 1884: 2, 295: 2, 2338: 0, 1131: 0, 1360: 1, 449: 0, 2311: 0, 73: 0, 2217: 2, 384: 2, 96: 0, 216: 0, 1527: 0, 1473: 2, 682: 2, 1301: 0, 734: 2, 1905: 2, 59: 2, 1187: 2, 1338: 2, 2335: 0, 1845: 2, 99: 0, 2324: 2, 1749: 2, 825: 2, 1127: 2, 783: 2, 486: 2, 1026: 2, 33: 0, 1347: 2, 421: 0, 2375: 0, 1136: 2, 1511: 2, 716: 2, 1927: 2, 1944: 2, 473: 1, 1950: 2, 58: 0, 727: 0, 222: 2, 400: 0, 1246: 2, 533: 2, 2332: 2, 1424: 0, 1032: 2, 558: 1, 512: 2, 724: 2, 797: 2, 2403: 0, 676: 1, 2388: 0, 1794: 0, 246: 2, 2056: 2, 584: 0, 1486: 0, 1638: 1, 460: 2, 1811: 0, 1916: 0, 1978: 2, 946: 0, 2340: 1, 1681: 2, 1721: 0, 229: 2, 1072: 2, 1696: 0, 1988: 2, 169: 2, 572: 1, 2224: 1, 753: 0, 1252: 1, 2342: 0, 607: 2, 436: 0, 664: 2, 268: 1, 799: 2, 586: 0, 2114: 2, 427: 0, 2181: 0, 355: 2, 2214: 0, 1783: 1, 90: 2, 1180: 2, 670: 2, 844: 2, 1524: 1, 2280: 1, 1746: 2, 2395: 0, 1792: 2, 1106: 0, 1173: 2, 2012: 2, 1114: 1, 1872: 0, 548: 0, 1628: 0, 769: 2, 2269: 0, 2240: 2, 2040: 0, 1405: 2, 1134: 0, 667: 0, 2160: 0, 2347: 0, 1132: 0, 348: 0, 2132: 2, 538: 1, 1577: 2, 471: 2, 914: 2, 955: 2, 1852: 2, 345: 0, 272: 1, 437: 2, 1675: 2, 1182: 2, 868: 0, 1821: 0, 560: 0, 733: 2, 2390: 0, 1788: 2, 852: 2, 2014: 2, 1873: 2, 444: 2, 354: 2, 488: 2, 1767: 0, 1999: 2, 360: 2, 245: 2, 1720: 0, 629: 0, 1130: 2, 608: 0, 537: 2, 368: 2, 988: 2, 1854: 2, 1807: 2, 334: 0, 535: 0, 1608: 2, 732: 2, 1935: 1, 495: 2, 547: 2, 665: 0, 970: 1, 1941: 2, 2080: 2, 243: 0, 224: 2, 387: 2, 397: 0, 611: 2, 2239: 2, 2339: 0, 361: 1, 1932: 2, 1573: 2, 483: 2, 1248: 2, 961: 2, 656: 0, 439: 1, 1435: 0, 1639: 0, 2167: 0, 1176: 2, 2086: 2, 1747: 2, 273: 1, 1523: 2, 1596: 2, 905: 2, 1732: 0, 1748: 2, 1683: 0, 57: 0, 369: 2, 130: 0, 1808: 2, 147: 2, 631: 0, 2394: 2, 1100: 2, 776: 2, 1593: 2, 780: 2, 95: 0, 1281: 1, 2076: 0, 2270: 0, 1931: 2, 610: 2, 2281: 0, 2405: 0, 2380: 0, 1844: 2, 416: 0, 1409: 2, 882: 2, 1031: 2, 1809: 0, 782: 2, 2042: 2, 1023: 2, 1574: 0, 723: 2, 1186: 2, 1965: 1, 1283: 2, 1115: 0, 1818: 0, 2090: 2, 2001: 0, 1242: 2, 627: 0, 1785: 0, 2253: 0, 1373: 2, 1116: 0, 282: 0, 1937: 2, 72: 2, 800: 2, 1962: 0, 2266: 2, 1384: 2, 1981: 1, 1022: 2, 164: 2, 1379: 2, 494: 2, 2089: 2, 2078: 2, 1518: 2, 1133: 0, 1088: 2, 573: 2, 1843: 2, 1911: 2, 2226: 0, 1344: 0, 1179: 2, 231: 2, 2238: 2, 68: 2, 1874: 2, 298: 0, 153: 2, 1517: 2, 1228: 1, 228: 2, 1936: 2, 545: 2, 1961: 2, 550: 2, 1883: 2, 34: 1, 1223: 1, 1463: 2, 874: 2, 933: 2, 1940: 2, 446: 2, 798: 2, 466: 1, 62: 0, 1274: 2, 2221: 0, 1610: 2, 2168: 0, 1520: 2, 1997: 2, 666: 0, 451: 2, 1719: 0, 2374: 0, 2386: 0, 1243: 0, 759: 2, 2007: 1, 1838: 2, 417: 0, 557: 2, 1156: 2, 552: 2, 476: 2, 710: 2, 242: 0, 785: 2, 1469: 2, 1181: 2, 2247: 0, 393: 0, 1202: 0, 583: 0, 2205: 2, 1870: 2, 1340: 2, 461: 1, 1994: 2, 856: 2, 2087: 2, 283: 0, 1588: 2, 1790: 0, 1640: 0, 1315: 2, 1951: 2, 2219: 0, 895: 2, 2376: 0, 2393: 2, 1823: 1, 377: 0, 1024: 0, 386: 1, 944: 2, 795: 2, 1629: 1, 175: 2, 428: 0, 2271: 0, 1993: 2, 2273: 0, 1080: 2, 869: 0, 1611: 0, 1314: 0, 1769: 0, 754: 2, 269: 2, 2075: 1, 1084: 2, 532: 0, 125: 0, 1175: 2, 541: 0, 2207: 1, 606: 2, 2262: 2, 2279: 1, 569: 1, 508: 2, 726: 1, 2107: 1, 2413: 2, 2349: 2, 1770: 2, 1087: 2, 543: 2, 778: 2, 389: 2, 2310: 2, 55: 2, 2341: 0, 177: 2, 1875: 2, 1942: 0, 1871: 2, 218: 2, 908: 0, 385: 2, 1408: 2, 1912: 2, 1251: 0, 1677: 2, 968: 2, 453: 0, 2352: 0, 719: 2, 2295: 0, 487: 2, 438: 2, 2035: 2, 1810: 0, 1030: 2, 2263: 0, 1249: 2, 1137: 2, 2337: 0, 1388: 0, 2134: 0, 221: 2, 2334: 0, 225: 2, 1317: 2, 346: 0, 1128: 2, 846: 2, 344: 0, 1060: 2, 1477: 1, 1120: 0, 1904: 2, 1525: 2, 1552: 2, 2296: 0, 669: 0, 301: 2, 987: 2, 430: 0, 396: 0, 2020: 2, 2265: 0, 721: 2, 1184: 2, 767: 2, 2000: 2, 2190: 2, 1817: 0, 299: 0, 305: 0, 53: 2, 1399: 0, 182: 1, 1949: 2, 567: 0, 1057: 2, 1587: 2, 419: 1, 2153: 0, 546: 2, 745: 2, 2377: 0, 1442: 2, 1380: 2, 1591: 2, 89: 1, 674: 0, 1025: 2, 2222: 0, 1277: 0, 2328: 0, 100: 0, 30: 2, 1789: 0, 663: 0, 69: 2, 399: 2, 1309: 2, 61: 2, 1842: 2, 146: 2, 454: 0, 1339: 0, 315: 0, 1404: 0, 661: 0, 309: 0, 671: 2, 762: 2, 1676: 2, 74: 0, 375: 2, 1803: 2, 465: 0, 1391: 0, 1441: 2, 1840: 2, 150: 2, 56: 2, 278: 2, 91: 2, 1402: 1, 244: 0, 1126: 2, 178: 2, 1221: 2, 219: 2, 332: 0, 568: 1, 2028: 2, 1081: 2, 126: 2, 2387: 0, 927: 2, 131: 0, 556: 2, 97: 0, 1484: 0, 2112: 2, 2126: 0, 1998: 2, 1227: 2, 312: 0, 356: 2, 1470: 2, 163: 2, 2192: 0, 333: 0, 1805: 0, 2415: 0, 36: 0, 1820: 2, 2059: 2, 570: 2, 1361: 1, 1584: 2, 2025: 2, 960: 2, 1576: 0, 2128: 1, 1594: 2, 2392: 0, 2343: 2, 77: 0, 1075: 2, 300: 0, 1348: 2, 485: 2, 1867: 2, 1979: 2, 1562: 2, 571: 1, 892: 2, 340: 0, 1109: 2, 1135: 2, 1155: 0, 2104: 2, 1521: 0, 1561: 2, 929: 2, 921: 2, 871: 0, 1694: 0, 1278: 0, 484: 2, 1903: 2, 973: 2, 925: 2, 1282: 2, 1902: 2, 2329: 0, 2389: 2, 2094: 2, 1112: 0, 1784: 0, 845: 2, 1909: 2, 71: 2, 179: 2, 1411: 2, 1170: 2, 2085: 2, 1038: 2, 452: 0, 1121: 1, 2123: 0, 823: 0, 226: 2, 2344: 0, 1102: 2, 1679: 2, 443: 0, 1476: 2, 31: 2, 1512: 2, 2225: 2, 1183: 2, 223: 2, 1485: 0, 1553: 2, 2331: 0, 1191: 2, 1279: 2, 311: 1, 922: 2, 2404: 0, 2005: 0, 1171: 0, 148: 0, 714: 2, 1444: 2, 1462: 2, 1117: 2, 1185: 2, 1308: 0, 1111: 0, 2084: 1, 1073: 2, 924: 2, 98: 0, 2140: 1, 566: 2, 230: 2, 2137: 0, 752: 0, 76: 2, 373: 0, 1797: 0, 928: 2, 1980: 2, 912: 0, 534: 2, 1358: 2, 779: 2, 539: 0, 2326: 2, 2325: 2, 2283: 0, 1074: 2, 342: 1, 958: 2, 2419: 0, 25: 0, 2083: 2, 445: 0, 553: 2, 294: 0, 1522: 2, 1250: 2, 1963: 0, 870: 0, 2197: 1, 1359: 1, 2418: 0, 2057: 2, 1560: 1, 621: 2, 1841: 2, 2191: 2, 162: 2, 930: 2, 2091: 2, 1341: 0, 165: 2, 475: 2, 357: 2, 1172: 2, 1037: 0, 60: 2, 2109: 2, 1229: 0, 1103: 0, 963: 2, 1514: 2, 2420: 0, 1245: 2, 2218: 1, 2237: 2, 1244: 0, 1337: 0, 401: 0, 1934: 1, 418: 0, 722: 2, 1929: 2, 910: 0, 1048: 0, 1382: 2, 1443: 2, 824: 2, 22: 0, 1129: 2, 2201: 0, 675: 2, 2312: 0, 1487: 2, 1765: 0, 1398: 0, 975: 2, 623: 2, 1076: 2, 715: 2, 1159: 0, 2378: 0, 1410: 2, 1230: 2, 2038: 2, 768: 1, 398: 0, 630: 1, 2096: 0, 2017: 2, 605: 2, 1541: 2, 2385: 0, 2003: 0, 2054: 2, 1886: 0, 378: 2, 2004: 0, 302: 0, 2174: 0, 559: 2, 1728: 1, 306: 0, 2093: 2, 2250: 0, 2113: 2, 1513: 2, 1431: 0, 1913: 1, 2037: 2, 2227: 0, 1194: 2, 2082: 1, 1104: 2, 1406: 2, 1549: 2, 2055: 2, 1806: 2, 1804: 2, 736: 2, 2350: 2, 281: 2, 1389: 0, 2152: 0, 2200: 1, 574: 0, 2414: 0, 1107: 2, 1982: 1, 2297: 0, 450: 0, 2103: 2, 349: 0, 1630: 0, 1799: 2, 601: 1, 2396: 0, 2183: 0, 622: 2, 2216: 2, 2336: 0, 1192: 2, 585: 0, 247: 0, 1519: 2, 271: 2, 1110: 2, 374: 0, 388: 2, 2228: 0, 168: 2, 276: 2, 1050: 0, 1800: 0, 2155: 0, 1787: 2, 2162: 0, 1077: 2, 1976: 2, 296: 2, 152: 2, 872: 0, 2323: 0, 1189: 2, 2039: 2, 2024: 2, 1595: 2, 2213: 2, 1407: 2, 2125: 0, 2272: 2, 2019: 2, 2105: 2, 2110: 2, 422: 2, 1795: 0, 893: 2, 1586: 1, 796: 2, 1751: 2, 1554: 2, 1915: 1, 1678: 2, 2333: 0, 2034: 2, 2255: 0, 2246: 0, 855: 2, 725: 2, 2133: 1, 1471: 2, 662: 2, 1319: 0, 2088: 2, 2348: 0, 1791: 2, 613: 2, 1564: 2, 932: 2, 1086: 1, 2124: 0, 1865: 2, 1118: 0, 1964: 0, 1285: 2, 280: 2, 510: 2, 956: 2, 2248: 1, 1275: 0, 2206: 0, 775: 2, 2175: 0, 909: 0, 127: 0, 2164: 0, 1984: 1, 1316: 0, 969: 2, 274: 2, 180: 2, 54: 0, 307: 0, 492: 2, 540: 0, 2108: 2, 509: 2, 743: 0, 1436: 2, 1346: 0, 1876: 2, 1693: 0, 87: 2, 2252: 0, 1468: 2, 493: 2, 248: 2, 1105: 2, 2185: 0, 2161: 0, 390: 2, 2136: 0, 2163: 0, 1866: 2, 2417: 0, 358: 2, 275: 1, 215: 1, 1966: 0, 913: 0, 876: 0, 2261: 0, 1400: 0, 709: 1, 1371: 2, 279: 2, 965: 2, 1816: 0, 1855: 2, 1730: 0, 708: 2, 1695: 0, 420: 0, 758: 2, 220: 2, 23: 2, 1914: 1, 1977: 0, 2220: 0, 873: 1, 2115: 2, 1342: 0, 1079: 2, 24: 0, 1472: 1, 1193: 2, 1798: 2, 1901: 2, 448: 2, 959: 0, 1641: 0, 2095: 1, 176: 2, 2097: 1, 1390: 0, 1863: 2, 435: 0, 1533: 1, 167: 2, 2026: 2, 2135: 1, 1401: 1, 1083: 2, 1824: 2, 1393: 0}
